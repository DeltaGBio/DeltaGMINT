####
# This script is used to train a MLP head on the embeddings of a sequence pair generated by MINT model (finetuned ESM2 model).
# It is used to predict the difference in binding affinity between a wildtype and a mutant.
####
import os, random, json, math, time, argparse, re
from collections import OrderedDict
from datetime import datetime

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
import lightning.pytorch as pl
from lightning.pytorch.strategies import DDPStrategy
from lightning.pytorch.loggers import WandbLogger
import wandb
from sklearn.metrics import r2_score
from scipy.stats import pearsonr

import mint
from mint.model.esm2 import ESM2
from peft import get_peft_model, LoraConfig



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----------------------------
# Configuration
# ----------------------------
class CFG:
    """Global config for training and model."""
    EPOCHS = 1000
    BATCH_SIZE = 32
    LR = 1e-3
    WD = 1e-6
    SEED = 2024
    EMBED_DIM = 1280
    OUTPUT_DIM = 1
    RANK = 256
    PATIENCE = 40

if int(os.environ.get("RANK", "0")) == 0:
    wandb_logger = WandbLogger(project="DeltaG MINT Fuzzy", log_model=True)



# ----------------------------
# Collate function for mutational PPI
# ----------------------------
class MutationalPPICollateFn:
    """Collate function for mutational PPI batches."""
    def __init__(self, truncation_seq_length=None):
        self.alphabet = mint.data.Alphabet.from_architecture("ESM-1b")
        self.truncation_seq_length = truncation_seq_length

    def __call__(self, batches):
        wt_ab, wt_ag, mut_ab, mut_ag, labels = zip(*batches)
        wt_chains = [self.convert(c) for c in [wt_ab, wt_ag]]
        wt_chain_ids = [
            torch.ones(c.shape, dtype=torch.int32) * i for i, c in enumerate(wt_chains)
        ]
        wt_chains = torch.cat(wt_chains, -1)
        wt_chain_ids = torch.cat(wt_chain_ids, -1)
        mut_chains = [self.convert(c) for c in [mut_ab, mut_ag]]
        mut_chain_ids = [
            torch.ones(c.shape, dtype=torch.int32) * i for i, c in enumerate(mut_chains)
        ]
        mut_chains = torch.cat(mut_chains, -1)
        mut_chain_ids = torch.cat(mut_chain_ids, -1)
        labels = torch.from_numpy(np.stack(labels, 0))
        return wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, labels

    def convert(self, seq_str_list):
        """Tokenize and pad a list of sequence strings."""
        batch_size = len(seq_str_list)
        seq_encoded_list = [
            self.alphabet.encode("<cls>" + seq_str.replace("J", "L") + "<eos>")
            for seq_str in seq_str_list
        ]
        if self.truncation_seq_length:
            for i in range(batch_size):
                seq = seq_encoded_list[i]
                if len(seq) > self.truncation_seq_length:
                    start = random.randint(0, len(seq) - self.truncation_seq_length + 1)
                    seq_encoded_list[i] = seq[start:start + self.truncation_seq_length]
        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)
        if self.truncation_seq_length:
            assert max_len <= self.truncation_seq_length
        tokens = torch.empty((batch_size, max_len), dtype=torch.int64)
        tokens.fill_(self.alphabet.padding_idx)
        for i, seq_encoded in enumerate(seq_encoded_list):
            seq = torch.tensor(seq_encoded, dtype=torch.int64)
            tokens[i, :len(seq_encoded)] = seq
        return tokens

# ----------------------------
# Dataset & Collate Function
# ----------------------------
class ProteinSequenceDataset(Dataset):
    """Dataset for protein sequence pairs and targets from CSV."""
    def __init__(self, df_path, col1, col2, col3, col4, target_col, test_run=False):
        super().__init__()
        self.df = pd.read_csv(df_path)
        if test_run:
            self.df = self.df.sample(n=101)
        self.seqs1 = self.df[col1].tolist()
        self.seqs2 = self.df[col2].tolist()
        self.seqs3 = self.df[col3].tolist()
        self.seqs4 = self.df[col4].tolist()
        if isinstance(target_col, list):
            self.targets = self.df[target_col].to_numpy()
        else:
            self.targets = self.df[target_col].tolist()

    def __len__(self):
        return len(self.seqs1)

    def __getitem__(self, index):
        return (
            self.seqs1[index],
            self.seqs2[index],
            self.seqs3[index],
            self.seqs4[index],
            self.targets[index],
        )

def get_sequences_by_chain(chain_ids, amino_acids):
    """Group amino acids by chain id and return as list of strings."""
    chains = [[] for _ in range(torch.max(chain_ids) + 1)]
    for chain_id, amino_acid in zip(chain_ids, amino_acids):
        chains[chain_id].append(amino_acid)
    return ["".join(chain) for chain in chains]


def upgrade_state_dict(state_dict):
    """Remove encoder prefixes from state dict keys."""
    prefixes = ["encoder.sentence_encoder.", "encoder."]
    pattern = re.compile("^" + "|".join(prefixes))
    return {pattern.sub("", name): param for name, param in state_dict.items()}

# ----------------------------
# Sequence Embedder
# ----------------------------
class SequenceEmbedder(nn.Module):
    """ESM2-based sequence embedder for paired chains."""
    def __init__(
        self,
        cfg,
        checkpoint_path,
        freeze_percent=0.0,
        use_multimer=True,
        sep_chains=True,
        device="cuda:0",
    ):
        super().__init__()
        self.cfg = cfg
        self.sep_chains = sep_chains
        self.model = ESM2(
            num_layers=cfg.encoder_layers,
            embed_dim=cfg.encoder_embed_dim,
            attention_heads=cfg.encoder_attention_heads,
            token_dropout=cfg.token_dropout,
            use_multimer=use_multimer,
        )
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        if use_multimer:
            new_checkpoint = OrderedDict(
                (key.replace("model.", ""), value)
                for key, value in checkpoint["state_dict"].items()
            )
            self.model.load_state_dict(new_checkpoint)
        else:
            new_checkpoint = upgrade_state_dict(checkpoint["model"])
            self.model.load_state_dict(new_checkpoint)
        total_layers = cfg.encoder_layers
        for name, param in self.model.named_parameters():
            if (
                "embed_tokens.weight" in name
                or "_norm_after" in name
                or "lm_head" in name
            ):
                param.requires_grad = False
            else:
                layer_num = name.split(".")[1]
                if int(layer_num) <= math.floor(total_layers * freeze_percent):
                    param.requires_grad = False

    def get_one_chain(self, chain_out, mask_expanded, mask):
        """Mean pooling over masked positions for a chain."""
        masked_chain_out = chain_out * mask_expanded
        sum_masked = masked_chain_out.sum(dim=1)
        mask_counts = mask.sum(dim=1, keepdim=True).float()
        mean_chain_out = sum_masked / mask_counts
        return mean_chain_out

    def forward_one(self, chains, chain_ids):
        """Embed a single wildtype or mutant chain pair."""
        mask = (
            (~chains.eq(self.model.cls_idx))
            & (~chains.eq(self.model.eos_idx))
            & (~chains.eq(self.model.padding_idx))
        )
        chain_out = self.model(
            chains, chain_ids, repr_layers=[33]
        )["representations"][33]
        if self.sep_chains:
            mask_chain_0 = (chain_ids.eq(0) & mask).unsqueeze(-1).expand_as(chain_out)
            mask_chain_1 = (chain_ids.eq(1) & mask).unsqueeze(-1).expand_as(chain_out)
            mean_chain_out_0 = self.get_one_chain(
                chain_out, mask_chain_0, (chain_ids.eq(0) & mask)
            )
            mean_chain_out_1 = self.get_one_chain(
                chain_out, mask_chain_1, (chain_ids.eq(1) & mask)
            )
            return torch.cat((mean_chain_out_0, mean_chain_out_1), -1)
        else:
            mask_expanded = mask.unsqueeze(-1).expand_as(chain_out)
            masked_chain_out = chain_out * mask_expanded
            sum_masked = masked_chain_out.sum(dim=1)
            mask_counts = mask.sum(dim=1, keepdim=True).float()
            mean_chain_out = sum_masked / mask_counts
            return mean_chain_out

    def forward(self, wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, cat=False):
        """Embed wildtype and mutant, return difference or concat."""
        wt_chains_out = self.forward_one(wt_chains, wt_chain_ids)
        mut_chains_out = self.forward_one(mut_chains, mut_chain_ids)
        if cat:
            return torch.cat((wt_chains_out, mut_chains_out), -1)
        return wt_chains_out - mut_chains_out

@torch.no_grad()
def get_embeddings_two(model, loader, device, cat):
    """Extract embeddings and targets from loader using model."""
    embeddings = []
    targets = []
    for _, eval_batch in enumerate(loader):
        wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, target = eval_batch
        wt_chains = wt_chains.to(device)
        wt_chain_ids = wt_chain_ids.to(device)
        mut_chains = mut_chains.to(device)
        mut_chain_ids = mut_chain_ids.to(device)
        embedding = model(
            wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, cat
        )
        embeddings.append(embedding.detach().cpu())
        targets.append(target)
    embeddings = torch.cat(embeddings)
    targets = torch.cat(targets)
    targets = targets.reshape(-1, 1)
    return embeddings, targets

# ----------------------------
# MLP Head Definition
# ----------------------------
class MLP(nn.Module):
    """Multi-layer perceptron regression head."""
    def __init__(
        self,
        input_dim=CFG.EMBED_DIM,
        output_dim=CFG.OUTPUT_DIM,
        lr=CFG.LR,
        weight_decay=CFG.WD,
    ):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.lr = lr
        self.weight_decay = weight_decay
        self.fc1 = nn.Linear(self.input_dim, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.dropout = nn.Dropout(0.1)
        self.fc3 = nn.Linear(512, 256)
        self.output = nn.Linear(256, self.output_dim)
        self.initialize_weights()

    def initialize_weights(self):
        """Xavier initialization for all linear layers."""
        for m in self.modules():
            if isinstance(m, (nn.Linear, nn.Conv1d)):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = x.float()
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.output(x)
        return x

# ----------------------------
# Combined Model
# ----------------------------
class CombinedModel(nn.Module):
    """Model: embedder + MLP head for regression."""
    def __init__(self, embedder, mlp):
        super().__init__()
        self.embedder = embedder
        self.mlp = mlp

    def forward(self, wt_chains, wt_chain_ids, mut_chains, mut_chain_ids):
        embeddings = self.embedder(wt_chains, wt_chain_ids, mut_chains, mut_chain_ids)
        output = self.mlp(embeddings)
        return output

class DeltaGLightningModule(pl.LightningModule):
    """LightningModule for DeltaG regression using CombinedModel."""
    def __init__(self, model, loss_fn, lr, weight_decay=CFG.WD, gamma=0.95):
        super().__init__()
        self.model = model
        self.loss_fn = loss_fn
        self.lr = lr
        self.weight_decay = weight_decay
        self.gamma = gamma

    def forward(self, wt_chains, wt_chain_ids, mut_chains, mut_chain_ids):
        return self.model(wt_chains, wt_chain_ids, mut_chains, mut_chain_ids)

    def training_step(self, batch, batch_idx):
        wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, target = batch
        target = target.unsqueeze(-1).float()
        preds = self(wt_chains, wt_chain_ids, mut_chains, mut_chain_ids)
        loss = self.loss_fn(preds, target)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)
        return loss
    def on_train_epoch_start(self):
        self.train_outputs = []

    def on_train_epoch_end(self):   
        self.train_outputs.clear()

    def validation_step(self, batch, batch_idx):
        wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, target = batch
        target = target.unsqueeze(-1).float()
        preds = self(wt_chains, wt_chain_ids, mut_chains, mut_chain_ids)
        loss = self.loss_fn(preds, target)
        output = {'val_loss': loss, 'preds': preds, 'targets': target}
        if not hasattr(self, '_val_outputs'):
            self._val_outputs = []
        self._val_outputs.append(output)
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)
        return output

    def on_validation_epoch_start(self):
        self._val_outputs = []

    def on_validation_epoch_end(self):
        outputs = self._val_outputs
        if outputs:
            preds = torch.cat([x['preds'] for x in outputs], dim=0)
            targets = torch.cat([x['targets'] for x in outputs], dim=0)
            preds_np = preds.detach().cpu().numpy()
            targets_np = targets.detach().cpu().numpy()
            r2 = r2_score(targets_np[:, 0], preds_np[:, 0])
            pearson = pearsonr(targets_np[:, 0], preds_np[:, 0])[0]
            self.log('val_r2', r2, prog_bar=True, sync_dist=True)
            self.log('val_pearson', pearson, prog_bar=True, sync_dist=True)
                        
            #plot test results
            fig, ax = plt.subplots(figsize=(5, 5))
            sns.regplot(x=preds_np[:, 0], y=targets_np[:, 0], ax=ax, scatter_kws={'s': 50, 'alpha': 0.6}, line_kws={'color': 'red', 'lw': 2})

            # Plot y=x reference line
            min_val = min(targets_np[:, 0].min(), preds_np[:, 0].min())
            max_val = max(targets_np[:, 0].max(), preds_np[:, 0].max())
            ax.plot([min_val, max_val], [min_val, max_val], '--', color='gray', label='Ideal')


            ax.set_xlabel('Predicted Values')
            ax.set_ylabel('True Values')
            ax.set_title(f'Validation Results | R²: {r2:.2f} | Pearson: {pearson:.2f}')
            plt.tight_layout()
            wandb_logger.log_image(key="Validation set results", images=[wandb.Image(fig)], step=self.global_step)

        self._val_outputs.clear()

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.gamma)
        return [optimizer], [{'scheduler': scheduler, 'interval': 'epoch'}]
    
    def test_step(self, batch, batch_idx):
        wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, target = batch
        target = target.unsqueeze(-1).float()
        preds = self(wt_chains, wt_chain_ids, mut_chains, mut_chain_ids)
        output = {'preds': preds, 'targets': target}
        if not hasattr(self, '_test_outputs'):
            self._test_outputs = []
        self._test_outputs.append(output)
        return output
    
    def on_test_epoch_start(self):
        self._test_outputs = []

    def on_test_epoch_end(self):
        outputs = self._test_outputs
        if outputs:
            preds = torch.cat([x['preds'] for x in outputs], dim=0)
            targets = torch.cat([x['targets'] for x in outputs], dim=0)
            preds_np = preds.detach().cpu().numpy()
            targets_np = targets.detach().cpu().numpy()
            r2 = r2_score(targets_np[:, 0], preds_np[:, 0])
            pearson = pearsonr(targets_np[:, 0], preds_np[:, 0])[0]
            self.log('test_r2', r2, prog_bar=True, sync_dist=True)
            self.log('test_pearson', pearson, prog_bar=True, sync_dist=True)
            
            #only plot for for the global step 
            #plot test results
            fig, ax = plt.subplots(figsize=(5, 5))
            sns.regplot(x=preds_np[:, 0], y=targets_np[:, 0], ax=ax, scatter_kws={'s': 50, 'alpha': 0.6}, line_kws={'color': 'red', 'lw': 2})

            # Plot y=x reference line
            min_val = min(targets_np[:, 0].min(), preds_np[:, 0].min())
            max_val = max(targets_np[:, 0].max(), preds_np[:, 0].max())
            ax.plot([min_val, max_val], [min_val, max_val], '--', color='gray', label='Ideal')

            ax.set_xlabel('True Values')
            ax.set_ylabel('Predicted Values')
            ax.set_title(f'Test Results | R²: {r2:.2f} | Pearson: {pearson:.2f}')
            plt.tight_layout()
            wandb_logger.log_image(key="Test set results", images=[wandb.Image(fig)], step=self.global_step)
            #log the preds and targets as a table
            data = pd.DataFrame({'preds': preds_np[:, 0], 'targets': targets_np[:, 0]})
            wandb_logger.log_table(key="Test set results", dataframe=data)
        self._test_outputs.clear()
    

# ----------------------------
# Main function
# ----------------------------
def main(args):
    """Main training loop and setup using Lightning Trainer."""
    
    # Set up results directory with timestamp
    current_date = datetime.now()
    date_string = current_date.strftime("%Y%m%d_%H%M")
    results_dir = f"{args.results_dir}/{date_string}/"
    os.makedirs(results_dir, exist_ok=True)

    #update CFG with output directory
    CFG.OUTPUT_DIR = results_dir

    # Wandb setup
    wandb.login(key="e3c083efff2130aa9d6577d63be9be67eb124e30")
    # Setup Wandb logger
    if int(os.environ.get("RANK", "0")) == 0:
        config_dict = {k: getattr(CFG, k) for k in ["EPOCHS", "BATCH_SIZE", "LR", "WD", "SEED", "EMBED_DIM", "OUTPUT_DIM", "RANK", "OUTPUT_DIR"]}
        wandb_logger = WandbLogger(project="DeltaG MINT Fuzzy", log_model=True, config=config_dict)
    else:
        wandb_logger = None

    torch.manual_seed(CFG.SEED)
    np.random.seed(CFG.SEED)
    random.seed(CFG.SEED)
    CONFIG_DICT_PATH = "/pasteur/appa/scratch/dvu/github/mint/data/esm2_t33_650M_UR50D.json"
    cfg = argparse.Namespace()
    with open(CONFIG_DICT_PATH) as f:
        cfg.__dict__.update(json.load(f))
    train_fl = args.train_set
    col1 = "seq1"
    col2 = "seq2"
    mut_col1 = "seq1_mut"
    mut_col2 = "seq2_mut"
    target_col = "target"
    
    # Generate dataset
    dataset = ProteinSequenceDataset(
        df_path=train_fl,
        col1=col1,
        col2=col2,
        col3=mut_col1,
        col4=mut_col2,
        target_col=target_col,
    )

    # Split dataset into train and validation
    total_samples = len(dataset)
    indices = list(range(total_samples))
    random.shuffle(indices)
    train_split = int(0.8 * total_samples)
    train_indices, val_indices = indices[:train_split], indices[train_split:]
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=CFG.BATCH_SIZE,
        shuffle=True,
        collate_fn=MutationalPPICollateFn(),
        num_workers=7
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=CFG.BATCH_SIZE,
        shuffle=False,
        collate_fn=MutationalPPICollateFn(),
        num_workers=7
    )

    # Initialize embedder
    embedder = SequenceEmbedder(
        cfg,
        args.checkpoint_path,
        freeze_percent=1.0,
        use_multimer=True,
        sep_chains=args.sep_chains,
        device=device
    )

    if args.use_lora:
        lora_config = LoraConfig(
            r=CFG.RANK,
            lora_alpha=1,
            lora_dropout=0.1,
            bias="none",
            target_modules=["query", "key", "value", "dense"]
        )
        embedder.model = get_peft_model(embedder.model, lora_config)
        print("LoRA fine-tuning enabled. Trainable parameters:")
        print(embedder.model.print_trainable_parameters())

    # Initialize MLP head
    mlp_head = MLP(
        input_dim=CFG.EMBED_DIM,
        output_dim=CFG.OUTPUT_DIM,
        lr=CFG.LR,
        weight_decay=CFG.WD,
    )

    # Initialize combined model
    model = CombinedModel(embedder, mlp_head)
    model = model.to(device)
    loss_fn = torch.nn.HuberLoss()

    # Create LightningModule instance
    lightning_model = DeltaGLightningModule(model, loss_fn, lr=CFG.LR, weight_decay=CFG.WD, gamma=0.95)

    # Setup Lightning callbacks
    checkpoint_callback = pl.callbacks.ModelCheckpoint(
        monitor='val_loss',
        dirpath=results_dir,
        filename='epoch_{epoch}_model',
        save_top_k=1,
        mode='min'
    )
    early_stop_callback = pl.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=CFG.PATIENCE,
        verbose=True,
        mode='min'
    )
    lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')



    # Initialize Trainer
    trainer = pl.Trainer(
        max_epochs=CFG.EPOCHS,
        callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],
        logger=wandb_logger,
        accelerator='auto',
        strategy=DDPStrategy(find_unused_parameters=True)
    )

    # Fit the model using Lightning Trainer
    trainer.fit(lightning_model, train_loader, val_loader)

    # Test the model using a separate test set after training
    if args.test_set:
        test_dataset = ProteinSequenceDataset(
            df_path=args.test_set,
            col1=col1,
            col2=col2,
            col3=mut_col1,
            col4=mut_col2,
            target_col=target_col,
        )
        test_loader = DataLoader(
            test_dataset,
            batch_size=CFG.BATCH_SIZE,
            shuffle=False,
            collate_fn=MutationalPPICollateFn(),
            num_workers=7
        )
    
        
        test_trainer = pl.Trainer(devices=1, num_nodes=1, accelerator="auto", logger=wandb_logger)
        test_trainer.test(lightning_model, test_loader)



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train_set",
        type=str,
        default="datasets/criptc_norm_for_mochi_balanced_exp10_processed_MINT.csv",
        help="Path to CSV file containing 'sequence' and 'target' columns",
    )

    parser.add_argument(
        "--test_set",
        type=str,
        help="Path to test set CSV file",
    )
    parser.add_argument(
        "--results_dir",
        type=str,
        default="training",
        help="Path to results directory",
    )
    parser.add_argument(
        "--checkpoint_path",
        type=str,
        default="/pasteur/appa/scratch/dvu/github/mint/models/mint.ckpt",
        help="Path to MINT model checkpoint",
    )
    parser.add_argument(
        "--sep_chains",
        type=bool,
        default=False,
        help="Whether to separate chains",
    )
    parser.add_argument(
        "--cat",
        type=bool,
        default=False,
        help="Whether to concatenate embeddings",
    )
    parser.add_argument(
        "--use_lora",
        action="store_true",
        help="Enable LoRA fine-tuning on the embedding model",
        default=True,
    )
    args = parser.parse_args()
    main(args)