####
# This script is used to train a MLP head on the embeddings of a sequence pair generated by MINT model (finetuned ESM2 model).
# It is used to predict the difference in binding affinity between a wildtype and a mutant.
####
import os
import random
import json
import math
import time
import argparse
from collections import OrderedDict
from datetime import datetime

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
import wandb
import re
import mint
from mint.model.esm2 import ESM2
from peft import get_peft_model, LoraConfig
from tqdm import trange  
# Set up results directory with timestamp
current_date = datetime.now()
date_string = current_date.strftime("%Y%m%d_%H%M")
results_dir = f"results/{date_string}/"
os.makedirs(results_dir, exist_ok=True)

# Device selection
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----------------------------
# Configuration
# ----------------------------
class CFG:
    """Global config for training and model."""
    EPOCHS = 1000
    BATCH_SIZE = 32
    LR = 1e-3
    WD = 1e-6
    SEED = 2024
    EMBED_DIM = 1280
    OUTPUT_DIM = 1
    RANK = 16



# Wandb setup
wandb.login(key="e3c083efff2130aa9d6577d63be9be67eb124e30")
wandb.init(project="DeltaG MINT Fuzzy")
# Log CFG to wandb for experiment tracking and reproducibility
wandb.config.update({
    "EPOCHS": CFG.EPOCHS,
    "BATCH_SIZE": CFG.BATCH_SIZE,
    "LR": CFG.LR,
    "WD": CFG.WD,
    "SEED": CFG.SEED,
    "EMBED_DIM": CFG.EMBED_DIM,
    "OUTPUT_DIM": CFG.OUTPUT_DIM,
    "RANK": CFG.RANK,
})



# ----------------------------
# Collate function for mutational PPI
# ----------------------------
class MutationalPPICollateFn:
    """Collate function for mutational PPI batches."""
    def __init__(self, truncation_seq_length=None):
        self.alphabet = mint.data.Alphabet.from_architecture("ESM-1b")
        self.truncation_seq_length = truncation_seq_length

    def __call__(self, batches):
        wt_ab, wt_ag, mut_ab, mut_ag, labels = zip(*batches)
        wt_chains = [self.convert(c) for c in [wt_ab, wt_ag]]
        wt_chain_ids = [
            torch.ones(c.shape, dtype=torch.int32) * i for i, c in enumerate(wt_chains)
        ]
        wt_chains = torch.cat(wt_chains, -1)
        wt_chain_ids = torch.cat(wt_chain_ids, -1)
        mut_chains = [self.convert(c) for c in [mut_ab, mut_ag]]
        mut_chain_ids = [
            torch.ones(c.shape, dtype=torch.int32) * i for i, c in enumerate(mut_chains)
        ]
        mut_chains = torch.cat(mut_chains, -1)
        mut_chain_ids = torch.cat(mut_chain_ids, -1)
        labels = torch.from_numpy(np.stack(labels, 0))
        return wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, labels

    def convert(self, seq_str_list):
        """Tokenize and pad a list of sequence strings."""
        batch_size = len(seq_str_list)
        seq_encoded_list = [
            self.alphabet.encode("<cls>" + seq_str.replace("J", "L") + "<eos>")
            for seq_str in seq_str_list
        ]
        if self.truncation_seq_length:
            for i in range(batch_size):
                seq = seq_encoded_list[i]
                if len(seq) > self.truncation_seq_length:
                    start = random.randint(0, len(seq) - self.truncation_seq_length + 1)
                    seq_encoded_list[i] = seq[start:start + self.truncation_seq_length]
        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)
        if self.truncation_seq_length:
            assert max_len <= self.truncation_seq_length
        tokens = torch.empty((batch_size, max_len), dtype=torch.int64)
        tokens.fill_(self.alphabet.padding_idx)
        for i, seq_encoded in enumerate(seq_encoded_list):
            seq = torch.tensor(seq_encoded, dtype=torch.int64)
            tokens[i, :len(seq_encoded)] = seq
        return tokens

# ----------------------------
# Dataset & Collate Function
# ----------------------------
class ProteinSequenceDataset(Dataset):
    """Dataset for protein sequence pairs and targets from CSV."""
    def __init__(self, df_path, col1, col2, col3, col4, target_col, test_run=False):
        super().__init__()
        self.df = pd.read_csv(df_path)
        if test_run:
            self.df = self.df.sample(n=101)
        self.seqs1 = self.df[col1].tolist()
        self.seqs2 = self.df[col2].tolist()
        self.seqs3 = self.df[col3].tolist()
        self.seqs4 = self.df[col4].tolist()
        if isinstance(target_col, list):
            self.targets = self.df[target_col].to_numpy()
        else:
            self.targets = self.df[target_col].tolist()

    def __len__(self):
        return len(self.seqs1)

    def __getitem__(self, index):
        return (
            self.seqs1[index],
            self.seqs2[index],
            self.seqs3[index],
            self.seqs4[index],
            self.targets[index],
        )

def get_sequences_by_chain(chain_ids, amino_acids):
    """Group amino acids by chain id and return as list of strings."""
    chains = [[] for _ in range(torch.max(chain_ids) + 1)]
    for chain_id, amino_acid in zip(chain_ids, amino_acids):
        chains[chain_id].append(amino_acid)
    return ["".join(chain) for chain in chains]


def upgrade_state_dict(state_dict):
    """Remove encoder prefixes from state dict keys."""
    prefixes = ["encoder.sentence_encoder.", "encoder."]
    pattern = re.compile("^" + "|".join(prefixes))
    return {pattern.sub("", name): param for name, param in state_dict.items()}

# ----------------------------
# Sequence Embedder
# ----------------------------
class SequenceEmbedder(nn.Module):
    """ESM2-based sequence embedder for paired chains."""
    def __init__(
        self,
        cfg,
        checkpoint_path,
        freeze_percent=0.0,
        use_multimer=True,
        sep_chains=True,
        device="cuda:0",
    ):
        super().__init__()
        self.cfg = cfg
        self.sep_chains = sep_chains
        self.model = ESM2(
            num_layers=cfg.encoder_layers,
            embed_dim=cfg.encoder_embed_dim,
            attention_heads=cfg.encoder_attention_heads,
            token_dropout=cfg.token_dropout,
            use_multimer=use_multimer,
        )
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        if use_multimer:
            new_checkpoint = OrderedDict(
                (key.replace("model.", ""), value)
                for key, value in checkpoint["state_dict"].items()
            )
            self.model.load_state_dict(new_checkpoint)
        else:
            new_checkpoint = upgrade_state_dict(checkpoint["model"])
            self.model.load_state_dict(new_checkpoint)
        total_layers = cfg.encoder_layers
        for name, param in self.model.named_parameters():
            if (
                "embed_tokens.weight" in name
                or "_norm_after" in name
                or "lm_head" in name
            ):
                param.requires_grad = False
            else:
                layer_num = name.split(".")[1]
                if int(layer_num) <= math.floor(total_layers * freeze_percent):
                    param.requires_grad = False

    def get_one_chain(self, chain_out, mask_expanded, mask):
        """Mean pooling over masked positions for a chain."""
        masked_chain_out = chain_out * mask_expanded
        sum_masked = masked_chain_out.sum(dim=1)
        mask_counts = mask.sum(dim=1, keepdim=True).float()
        mean_chain_out = sum_masked / mask_counts
        return mean_chain_out

    def forward_one(self, chains, chain_ids):
        """Embed a single wildtype or mutant chain pair."""
        mask = (
            (~chains.eq(self.model.cls_idx))
            & (~chains.eq(self.model.eos_idx))
            & (~chains.eq(self.model.padding_idx))
        )
        chain_out = self.model(
            chains, chain_ids, repr_layers=[33]
        )["representations"][33]
        if self.sep_chains:
            mask_chain_0 = (chain_ids.eq(0) & mask).unsqueeze(-1).expand_as(chain_out)
            mask_chain_1 = (chain_ids.eq(1) & mask).unsqueeze(-1).expand_as(chain_out)
            mean_chain_out_0 = self.get_one_chain(
                chain_out, mask_chain_0, (chain_ids.eq(0) & mask)
            )
            mean_chain_out_1 = self.get_one_chain(
                chain_out, mask_chain_1, (chain_ids.eq(1) & mask)
            )
            return torch.cat((mean_chain_out_0, mean_chain_out_1), -1)
        else:
            mask_expanded = mask.unsqueeze(-1).expand_as(chain_out)
            masked_chain_out = chain_out * mask_expanded
            sum_masked = masked_chain_out.sum(dim=1)
            mask_counts = mask.sum(dim=1, keepdim=True).float()
            mean_chain_out = sum_masked / mask_counts
            return mean_chain_out

    def forward(self, wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, cat=False):
        """Embed wildtype and mutant, return difference or concat."""
        wt_chains_out = self.forward_one(wt_chains, wt_chain_ids)
        mut_chains_out = self.forward_one(mut_chains, mut_chain_ids)
        if cat:
            return torch.cat((wt_chains_out, mut_chains_out), -1)
        return wt_chains_out - mut_chains_out

@torch.no_grad()
def get_embeddings_two(model, loader, device, cat):
    """Extract embeddings and targets from loader using model."""
    embeddings = []
    targets = []
    for _, eval_batch in enumerate(loader):
        wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, target = eval_batch
        wt_chains = wt_chains.to(device)
        wt_chain_ids = wt_chain_ids.to(device)
        mut_chains = mut_chains.to(device)
        mut_chain_ids = mut_chain_ids.to(device)
        embedding = model(
            wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, cat
        )
        embeddings.append(embedding.detach().cpu())
        targets.append(target)
    embeddings = torch.cat(embeddings)
    targets = torch.cat(targets)
    targets = targets.reshape(-1, 1)
    return embeddings, targets

# ----------------------------
# MLP Head Definition
# ----------------------------
class MLP(nn.Module):
    """Multi-layer perceptron regression head."""
    def __init__(
        self,
        input_dim=CFG.EMBED_DIM,
        output_dim=CFG.OUTPUT_DIM,
        lr=CFG.LR,
        weight_decay=CFG.WD,
    ):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.lr = lr
        self.weight_decay = weight_decay
        self.fc1 = nn.Linear(self.input_dim, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.dropout = nn.Dropout(0.1)
        self.fc3 = nn.Linear(512, 256)
        self.output = nn.Linear(256, self.output_dim)
        self.initialize_weights()

    def initialize_weights(self):
        """Xavier initialization for all linear layers."""
        for m in self.modules():
            if isinstance(m, (nn.Linear, nn.Conv1d)):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = x.float()
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.output(x)
        return x

# ----------------------------
# Combined Model
# ----------------------------
class CombinedModel(nn.Module):
    """Model: embedder + MLP head for regression."""
    def __init__(self, embedder, mlp):
        super().__init__()
        self.embedder = embedder
        self.mlp = mlp

    def forward(self, wt_chains, wt_chain_ids, mut_chains, mut_chain_ids):
        embeddings = self.embedder(wt_chains, wt_chain_ids, mut_chains, mut_chain_ids)
        output = self.mlp(embeddings)
        return output

# ----------------------------
# Metrics & Plotting
# ----------------------------
def calculate_metrics(y_pred, y_true, epoch, mode="Train"):
    """Compute and log R2, Pearson, and scatter plot."""
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    r2 = r2_score(y_true[:, 0], y_pred[:, 0])
    pearson = pearsonr(y_true[:, 0], y_pred[:, 0])[0]
    print(f"{mode} Epoch {epoch} | R2: {r2:.2f} | Pearson: {pearson:.2f}")
    wandb.log({f"{mode} R2": r2, f"{mode} Pearson": pearson}, step=epoch)
    fig, ax = plt.subplots(figsize=(5, 5))
    sns.regplot(
        x=y_true[:, 0],
        y=y_pred[:, 0],
        ax=ax,
        color="tab:blue",
        line_kws={"label": "Regression Line"},
        scatter_kws={"alpha": 0.5, "s": 5},
    )
    ax.set_xlabel("True Values")
    ax.set_ylabel("Predicted Values")
    ax.set_title(f"{mode} | R2: {r2:.2f} | Pearson: {pearson:.2f}")
    plt.tight_layout()
    wandb.log({f"{mode} Scatter": wandb.Image(plt)}, step=epoch)
    plt.close()

# ----------------------------
# Training & Testing Functions
# ----------------------------
def train_model(epoch, model, train_loader, optimizer, loss_fn):
    """Train for one epoch, log metrics."""
    model.train()
    all_preds, all_targets = [], []
    running_loss = 0.0
    steps = 0
    for wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, target in train_loader:
        optimizer.zero_grad()
        #to device
        wt_chains = wt_chains.to(device)
        wt_chain_ids = wt_chain_ids.to(device)
        mut_chains = mut_chains.to(device)
        mut_chain_ids = mut_chain_ids.to(device)
        target = target.to(device)
        target = target.unsqueeze(-1)
        preds = model(wt_chains, wt_chain_ids, mut_chains, mut_chain_ids)
        loss = loss_fn(preds, target.float())
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        steps += 1
        all_preds.append(preds.detach().cpu().numpy())
        all_targets.append(target.detach().cpu().numpy())
    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    calculate_metrics(all_preds, all_targets, epoch, mode="Train")
    return running_loss / steps

def test_model(epoch, model, test_loader, loss_fn):
    """Evaluate on validation/test set, log metrics."""
    model.eval()
    all_preds, all_targets = [], []
    running_loss = 0.0
    steps = 0
    with torch.no_grad():
        for wt_chains, wt_chain_ids, mut_chains, mut_chain_ids, target in test_loader:
            #to device
            wt_chains = wt_chains.to(device)
            wt_chain_ids = wt_chain_ids.to(device)
            mut_chains = mut_chains.to(device)
            mut_chain_ids = mut_chain_ids.to(device)
            target = target.to(device)
            target = target.unsqueeze(-1)
            preds = model(wt_chains, wt_chain_ids, mut_chains, mut_chain_ids)
            loss = loss_fn(preds, target.float())
            running_loss += loss.item()
            steps += 1
            all_preds.append(preds.cpu().numpy())
            all_targets.append(target.cpu().numpy())
    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    calculate_metrics(all_preds, all_targets, epoch, mode="Test")
    return running_loss / steps

# ----------------------------
# Main function
# ----------------------------
def main(args):
    """Main training loop and setup."""
    torch.manual_seed(CFG.SEED)
    np.random.seed(CFG.SEED)
    random.seed(CFG.SEED)
    CONFIG_DICT_PATH = "/pasteur/appa/scratch/dvu/github/mint/data/esm2_t33_650M_UR50D.json"
    cfg = argparse.Namespace()
    with open(CONFIG_DICT_PATH) as f:
        cfg.__dict__.update(json.load(f))
    train_fl = args.csv_file
    col1 = "seq1"
    col2 = "seq2"
    mut_col1 = "seq1_mut"
    mut_col2 = "seq2_mut"
    target_col = "target"
    
    #generate dataset
    dataset = ProteinSequenceDataset(
        df_path=train_fl,
        col1=col1,
        col2=col2,
        col3=mut_col1,
        col4=mut_col2,
        target_col=target_col,
    )

    #split dataset into train and validation
    total_samples = len(dataset)
    indices = list(range(total_samples))
    random.shuffle(indices)
    train_split = int(0.8 * total_samples)
    train_indices, val_indices = indices[:train_split], indices[train_split:]
    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)

    #create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=CFG.BATCH_SIZE,
        shuffle=True,
        collate_fn=MutationalPPICollateFn(),
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=CFG.BATCH_SIZE,
        shuffle=False,
        collate_fn=MutationalPPICollateFn(),
    )

    # Initialize embedder following PEP8 style guide
    embedder = SequenceEmbedder(
        cfg,
        args.checkpoint_path,
        freeze_percent=1.0,
        use_multimer=True,
        sep_chains=args.sep_chains,
        device=device
    )

    if args.use_lora:
        lora_config = LoraConfig(
            r=CFG.RANK,
            lora_alpha=1,
            lora_dropout=0.1,
            bias="none",
            target_modules=["query", "key", "value", "dense"]
        )
        embedder.model = get_peft_model(embedder.model, lora_config)
        print("LoRA fine-tuning enabled. Trainable parameters:")
        print(embedder.model.print_trainable_parameters())

    #Initialize MLP head
    mlp_head = MLP(
        input_dim=CFG.EMBED_DIM,
        output_dim=CFG.OUTPUT_DIM,
        lr=CFG.LR,
        weight_decay=CFG.WD,
    )

    #Initialize combined model
    model = CombinedModel(embedder, mlp_head)
    model = model.to(device)
    loss_fn = torch.nn.HuberLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.LR, weight_decay=CFG.WD)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)
    best_loss = float("inf")
    early_stopping_counter = 0
    num_epochs = CFG.EPOCHS

    for epoch in trange(num_epochs, desc="Epochs", ncols=80):
        start_time = time.time()
        train_loss = train_model(epoch, model, train_loader, optimizer, loss_fn)
        epoch_time = time.time() - start_time
        wandb.log({"Train Loss": train_loss, "Epoch Time (s)": epoch_time}, step=epoch)
        print(f"Epoch {epoch} | Train Loss: {train_loss:.4f}")
        if epoch % 10 == 0:
            val_loss = test_model(epoch, model, val_loader, loss_fn)
            wandb.log({"Validation Loss": val_loss}, step=epoch)
            print(f"Epoch {epoch} | Val Loss: {val_loss:.4f}")
            if val_loss < best_loss:
                best_loss = val_loss
                early_stopping_counter = 0
                save_path = os.path.join(results_dir, f"epoch_{epoch}_model.pth")
                torch.save(model.state_dict(), save_path)
                wandb.save(save_path)
            else:
                early_stopping_counter += 1
                if early_stopping_counter > 5:
                    print("Stopping early due to no improvement")
                    break
        scheduler.step()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--csv_file",
        type=str,
        default="datasets/criptc_norm_for_mochi_balanced_exp10_processed_MINT.csv",
        help="Path to CSV file containing 'sequence' and 'target' columns",
    )
    parser.add_argument(
        "--checkpoint_path",
        type=str,
        default="/pasteur/appa/scratch/dvu/github/mint/models/mint.ckpt",
        help="Path to MINT model checkpoint",
    )
    parser.add_argument(
        "--sep_chains",
        type=bool,
        default=False,
        help="Whether to separate chains",
    )
    parser.add_argument(
        "--cat",
        type=bool,
        default=False,
        help="Whether to concatenate embeddings",
    )
    parser.add_argument(
        "--use_lora",
        action="store_true",
        help="Enable LoRA fine-tuning on the embedding model",
        default=True,
    )
    args = parser.parse_args()
    main(args)